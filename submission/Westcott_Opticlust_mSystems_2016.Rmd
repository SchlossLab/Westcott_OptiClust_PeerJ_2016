---
title: "**OptiClust: Improved method for assigning amplicon-based sequence data to operational taxonomic units**"
bibliography: references.bib
output:
  pdf_document:
    includes:
      in_header: header.tex
csl: msystems.csl
fontsize: 11pt
geometry: margin=1.0in
---


```{r knitr_settings, eval=TRUE, echo=FALSE, cache=FALSE}
opts_chunk$set("tidy" = TRUE)
opts_chunk$set("echo" = FALSE)
opts_chunk$set("eval" = TRUE)
opts_chunk$set("warning" = FALSE)
opts_chunk$set("cache" = FALSE)

inline_hook <- function(x){
	print(x)
	if(is.numeric(x)){
		if(abs(x - round(x)) < .Machine$double.eps^0.5){
			paste(format(x,big.mark=',', digits=0, scientific=FALSE))
		} else {
			paste(format(x,big.mark=',', digits=2, nsmall=2, scientific=FALSE))
		}

	} else {
    	paste(x)      
	}
}
knitr::knit_hooks$set(inline=inline_hook)
```

\begin{center}
\vspace{25mm}
Sarah L. Westcott and Patrick D. Schloss${^\dagger}$

\vspace{30mm}

$\dagger$ To whom correspondence should be addressed: pschloss@umich.edu

Department of Microbiology and Immunology, University of Michigan, Ann Arbor, MI
\end{center}


\newpage
\linenumbers

## Abstract



\newpage

## Introduction

Amplicon-based sequencing has provided incredible insights into Earth's microbial biodiversity.

Numerous methods have been proposed

Needed an objective criteria to evaluate the various methods methods

Average neighbor is consistently the best method, but others including USEARCH and VSEARCH resulted in comparable results

Difficulty with amount of memory and time required to complete average neighbor algorithm is a significant hurdle to completion of the method.

Instead of retrospectively evaluating methods for their ability to correctly assign sequences we sought to develop a method that would prospectively assign sequences to OTUs to optimize classification metrics.




## Results

***OptiClust algorithm (Figure 1A).*** The OptiClust algorithm uses a list of all sequence names in the dataset, the pairs of sequences that are within a desired threshold of each other (e.g. 0.03), and the metric that should be used to assess clustering quality (e.g. MCC). The sequence pairs that are within the user-specified threshold are used to indicate those sequences that should appear within the same OTU and those that have a distance larger than the threshold should not appear within the same OTU. The algorithm starts by placing each sequence within its own OTU and calculating the number of true positives, true negatives, false positives, and false negatives based on the pairwise distances. These counts are used to calculate the metric used for optimization. The algorithm proceeds by interrogating each sequence and re-calculating the metric for the cases where the sequence stays in its current OTU, is moved to each of the other OTUs, or is moved into a new OTU. The location that results in the largest value of the optimization metric indicates whether the sequence remains in its current OTU or is moved. Each iteration consists of interrogating every sequence in the dataset. The algorithm continues until the optimization metric stabilizizes.

***OptiClust stopping criteria (Figure 1B).*** By default, the mothur-based implementation of the algorithm stops when the optimization metric does not change between iterations. It also allows the user to stop the algorithm if a maximum number of iterations is exceeded or if the change in the optimization metric is less than a stopping threshold. For each of the datasets we evaluated, between **XXX** and **XXX** iterations were needed to reach full conversion of the algorithm and between **XXX* and **XXX** iterations were needed to see a change in the metric that was less than 0.01%. To achieve full stabilization of the optimization metric, the **XXXX** algorithm required an average of **XX**% more iterations than the **XXXX** algorithm, which needed **XX**% more iterations than the **XXXX** algorithm. These results suggest that achieving full stabilization of the optimization metric adds computational effort, but that it is relatively small.

***OptiClust-generated OTUs are as stable as those from other algorithms (Figure 1C).*** One concern that many have with *de novo* clustering algorithms is that their output is sensitive to the initial order of the sequences. An additional concern with the OptiClust algorithm is that it may stabilize at a local optimum. To evaluate these concerns we compared the results obtained using ten randomizations of the order that sequences were given to the algorithm. Similar to our previous analysis, we found that the MCC values obtained from the replicate clusterings did not vary by more than **0.XX**% and were actually less variable than what we observed using the other algorithms. The variation observed in clustering quality suggests that the algorithm generates OTU assignments with subtle variation, but that the assignments are of a similar quality.

***Optimization of clustering using composite metrics significantly improves clustering quality (Figure 2, 3, 4).*** There are multiple metrics available to assess clustering quality. The mothur-based implementation of OptiClust allows the user to use the MCC, F1 score, accuracy, sensitivity, specificity, and the sum of true positives and negatives. The MCC, F1 score, and accuracy are preferred because each of them incorporates all four values from the confusion matrix while others only utilize two values. It is relatively straightforward to implement other metrics. **Comparison of MCC, accuracy, and F1 score to each other for each dataset using the full set of sequences.** Each metric outperformed the observed values for the other clustering algorithms indicating that regardless of the metric one uses to evaluate clustering quality, the OptiClust algorithm generates better OTU assignments than any of the other methods. The number of OTUs generated by an algorithm is often used as a metric for clustering quality. We did not observe a significant correlation between clustering quality as measured by MCC, F1 score, or accuracy and the number of OTUs generated by each algorithm. **Interestingly, the values of the MCC, F1 score, and accuracy generated when using each metric to optimize clustering were similar across implementations.** Based on these results and its previous use in the OTU assignment literature, the remainder of our analysis uses the MCC metric for optimization.

***Optimization-based clustering is faster and uses less memory than current methods (Figure 5).*** Although not as important as the quality of clustering, the amount of time and memory required to assign sequences to OTUs is a legitimate concern. **XXXX**, **XXXX**, and **XXXX** were not able to complete within 50 hours for the **XXXX** and **XXXX** datasets. The OptiClust algorithm finished **XXX** to **XXX**% faster than the next fastest algorithm. Similarly, **XXXX**, **XXXX**, and **XXXX** were not able to complete using less than 48 GB of RAM for the **XXXX** and **XXXX** datasets. The OptiClust algorithm used **XXX** to **XXX**% less RAM than the next fastest algorithm. To evaluate howt the speed and memory usage scaled with the number of sequences in the dataset, we measured the time required and maximum RAM usage to cluster 20, 40, 60, 80, and 100% of the unique sequences in each dataset by each algorithm. Using the full datasets, we observed that the speed and memory requirements of the OptiClust algorithm scaled **XXXXXXX**. These results clearly demonstrate that OptiClust is faster and uses less memory than other algorithms currently in use. This complements the higher quality OptiClust OTU assignments relative to other methods.

***Cluster splitting heuristic generates OTUs that are as good as non-split approach (Figure 6).*** We previously described a heuristic to accelerate OTU assignments where sequences were classified to taxonomic groupings and within each taxon sequences were assigned to OTUs. Using the average neighbor algorithm we found that this method was effective as clustering without using the classification information. We again assessed this approach using the average neighbor, VSEARCH abundance-based greedy clustering, and OptiClust by splitting the data at the family level and then clustering the sequences into OTUs. As before, we found that the quality of the clustering was as good using the classification data as without. An advantage of this approach is that because each taxon is distinct, it is possible to parallelize the assignment by clustering each taxon on a separate processor. Although the taxa are not equally abundant, there was still a considerable improvement in execution time and the amount of memory required to execute the analysis. The risk of this approach is that the initial classification may introduce error; however, with the datasets included in our analysis, this source of error did not appear to be significant.



## Discussion
Restate most important contributions
* Optimized clustering based on an objective criteria. Results are meaningfully better than existing methods
* Added benefits include smaller CPU and RAM footprint
* Result is efficient analysis of large datasets without sacrificing clustering quality as has been experienced in heuristic methods.


Choice of objective criteria
* Value of using a metric that is based on all four parameters
* Preference is for Matthew's Correlation Coefficient because... other options include F1 Score and accuracy


Preference for OptiClust over cluster splitting approach
* Risks are mis-classification and artificially splitting similar sequences between OTUs because they classify to different taxa
* Still potential to merge the methods for more efficient processing.


Based on our model, we propose the following...


## Materials and Methods

***Sequence data and processing steps.***
To evaluate the OptiClust and the other algorithms we created two synthetic sequence collections and four sequence collections generated from previously published studies. The V4 region of the 16S rRNA gene was used from all datasets because it is a popular region that can be fully sequenced with two-fold coverage using the commonly used MiSeq sequencer from Illumina [@Kozich2013]. The method for generating the simulated datasets followed the approach used by Kopylova et al. [-@Kopylova2016] and Schloss [@Schloss2016]. Briefly, we randomly selected 10,000 uniques V4 fragments from 16S rRNA gene sequences that were unique from the SILVA non-redundant database [@Pruesse2007]. A community with an even relative abundance profile was generated by specifying that each sequence had a frequency of 100 reads. A community with a staggered relative abundance profile was generated by specifying that the abundance of each sequence was a randomly drawn integer sampled from a uniform distribution between 1 and 200. Sequence collections collected from human feces [@Baxter2016], murine feces [@Schloss2012], soil [@Johnston2016], and seawater [@Henson2016] were used to characterize the algorithms' performance with natural communities. These sequence collections were all generated using paired 150 or 250 nt reads of the V4 region. We re-processed all of the reads using a common analysis pipeline that included quality score-based error correction [@Kozich2013], alignment against a SILVA reference database [@Pruesse2007; @Schloss2010], screening for chimeras using UCHIME [@Edgar2011], and classification using a naive Bayesian classifier with the RDP training set [@Wang2007].


***Implementation of clustering algorithms.***
In addition to the OptiClust algorithm we evaluated ten different *de novo* clustering algorithms. These included three hierarchical algorithms, average neighbor (AN), nearest neighbor (NN), and furthest neighbor (FN), which are implemented in mothur [v.1.39.0; @Schloss2009]. Seven heuristic methods were also used including abundance-based greedy clustering (UAGC) and distance-based greedy clustering (UDGC) as implemented in USEARCH [v.6.1; @Edgar2010], abundance-based greedy clustering (VAGC) and distance-based greedy clustering (VDGC) as implemented in VSEARCH [v.X.X.X; @Rognes2015], OTUClust [v.X.X.X; @XXXXXX], SumaClust [v.X.X.X; @XXXXXX], and Swarm [v.2.1.1; @Mah2014]. With the exception of Swarm each of these methods uses distance-based thresholds to report OTU assignments. To judge the quality of the Swarm-generated OTU assignments we calculated the MCC value using thresholds incremented by 1% between 0 and 5% and selected the threshold that provided the optimal MCC value [@Westcott2015]. We also assessed the ability of a previously describe heuristic to cluster sequences using the UDGC and OptiClust algorithms. In this heuristic sequences are split into bacterial families based on sequence classification and clustered within the taxonomic family [@Westcott20XX]. Finally, for our benchmarking analysis, we evaluate the memory and time requirements when using 8 processors with the UDGC and VDGC algorithms and with the taxonomic splitting heuristic.


***Benchmarking.*** We evaluated the quality of the sequence clustering, reproducibility of the clustering, the speed of clustering, and the amount of memory required to complete the clustering. To assess the quality of the clusters generated by each method, we counted the cells within a confusion matrix that indicated how well the clusterings represented the distances between the pair of sequences [@Schloss2011Assessing]. Pairs of sequences that were in the same OTU and had a distance less than 3% were true positives (TPs), those that were in different OTUs and had a distance greater than 3% were true negatives (TNs), those that were in the same OTU and had a distance greater than 3% were false positives (FPs), and those that were in different OTUs and had a distance less than 3% were false negatives (FNs). To synthesize the matrix into a single metric we used the Matthew's Correlation Coefficient, F1 score, and accuracy using the `sens.spec` command in mothur using the following equations.

$$
MCC = \frac{TP \times TN-FP \times FN}{\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)} }
$$

$$
F1_score = \frac{2 \times TP}{2 \times TP+FP+FN}
$$

$$
Accuracy = \frac{TP + TN}{(TP+FP+TN+FN)}
$$

To assess the reproducibility of the algorithms we randomized the starting order of each sequence collection ten times and ran each algorithm on each randomized collection. We then measured the MCC, F1 score, and accuracy for each randomization and quantified their coefficient of variation (CV; the ratio of the standard deviation to the mean).

To assess how the the memory and time requirements scaled with the number of sequences included in each sequence collection, we randomly subsampled 20, 40, 60, or 80% of the unique sequences in each collection. We obtained 10 subsamples at each depth for each dataset and ran each collection (N= 50 = 5 sequencing depths x 10 replicates) through each of the algorithms. We used the timeout script to quantify the maximum RAM used and the amount of time required to process each sequence collection (https://github.com/pshved/timeout). We limited each algorithm to 48 GB of RAM, 50 hours, and unless otherwise specified, a single processor.

***Data and code availability.*** The workflow utilized commands in GNU make (v.3.81), GNU bash (v.4.1.2), mothur [v.1.39.0; @Schloss2009], and R [v.`r paste(version$major, version$minor, sep='.')`; @language2015]. A reproducible version of this manuscript and analysis is available at https://github.com/SchlossLab/Westcott_OptiClust_mSystems_2015.


\newpage


## Figures

**Figure 1. The OptiClust algorithm.**
A. Flowchart describing algorithm with an example (?)
B. Plot of MCC/F1Score/Accuracy change with iterations
C. Plot of MCC/F1Score/Accuracy for the different datasets


**Figure 2.** MCC values for comparison of *de novo* clustering algorithms when applied to five natural and two synthetic datasets. For the purposes of this analysis, clustering was limited to 48 GB of RAM and 50 hours of execution time. The median of 10 re-orderings of the data is presented for each method and dataset.

**Figure 3.** F1-score values for comparison of *de novo* clustering algorithms when applied to five natural and two synthetic datasets. For the purposes of this analysis, clustering was limited to 48 GB of RAM and 50 hours of execution time. The median of 10 re-orderings of the data is presented for each method and dataset.

**Figure 4.** Accuracy values for comparison of *de novo* clustering algorithms when applied to five natural and two synthetic datasets. For the purposes of this analysis, clustering was limited to 48 GB of RAM and 50 hours of execution time. The median of 10 re-orderings of the data is presented for each method and dataset.

**Figure 5.** Demonstration of how execution time (A) and memory usage (B) scale with the number of unique sequences for each clustering algorithm. For the purposes of this analysis, clustering was limited to 48 GB of RAM and 50 hours of execution time.

**Figure 6.** Comparison of cluster.split and cluster for average neighbor, VSEARCH-based abundance-based greedy clustering, and OptiClust.


\newpage


## References
